// LLM Integration Service implementation
// Based on design document Line 695-702

import { ILLMIntegrationService, LLMUsageStats } from '../../types/services/llm.types';
import { ReplyContext } from '../../types/services/reply-assistant.types';
import { IChromeStorageRepository } from '../../types/infrastructure/storage.types';
import { STORAGE_KEYS } from '../../types/infrastructure/storage.types';

export class LLMIntegrationService implements ILLMIntegrationService {
  private usageStats: LLMUsageStats;
  
  constructor(private storageRepository: IChromeStorageRepository) {
    this.usageStats = {
      totalRequests: 0,
      totalTokens: 0,
      successRate: 1.0,
      averageResponseTime: 0,
      monthlyCost: 0,
      lastUpdated: new Date(),
    };
  }
  
  async generateReply(prompt: string, context: any): Promise<string> {
    const startTime = Date.now();
    
    try {
      // ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§LLMç”Ÿæˆ
      const reply = await this.generateReplyViaProxy(prompt, context);
      
      // Update usage statistics
      const responseTime = Date.now() - startTime;
      const estimatedTokens = Math.ceil(reply.length / 4); // Rough token estimation
      await this.updateUsageStats(true, responseTime, estimatedTokens);
      
      return reply;
    } catch (error) {
      console.error('LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼:', error);
      // ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ¢ãƒƒã‚¯ç”Ÿæˆã‚’ä½¿ç”¨
      const fallbackReply = await this.mockLLMGeneration(prompt, context);
      const responseTime = Date.now() - startTime;
      await this.updateUsageStats(false, responseTime, 0);
      return fallbackReply;
    }
  }
  
  optimizePrompt(context: ReplyContext): string {
    const { originalMessage, relatedMessages, userMapping, userPreferences } = context;
    
    let prompt = `Please generate a ${userPreferences.tone} reply to the following message:\n\n`;
    prompt += `Original Message:\n`;
    prompt += `From: ${originalMessage.from}\n`;
    prompt += `Channel: ${originalMessage.channel}\n`;
    prompt += `Content: ${originalMessage.content}\n\n`;
    
    if (userMapping) {
      prompt += `User Information:\n`;
      prompt += `Name: ${userMapping.name}\n`;
      prompt += `Tags: ${userMapping.tags.join(', ')}\n`;
      prompt += `Priority: ${userMapping.priority}\n\n`;
    }
    
    if (relatedMessages.length > 0) {
      prompt += `Related Messages:\n`;
      relatedMessages.forEach((msg, index) => {
        prompt += `${index + 1}. ${msg.content}\n`;
      });
      prompt += `\n`;
    }
    
    if (userPreferences.includeContext && context.conversationHistory.length > 0) {
      prompt += `Conversation History:\n`;
      context.conversationHistory.slice(-3).forEach((msg, index) => {
        prompt += `${index + 1}. ${msg.from}: ${msg.content}\n`;
      });
      prompt += `\n`;
    }
    
    prompt += `Please respond in ${userPreferences.language}.\n`;
    prompt += `Keep the response appropriate for ${originalMessage.channel} channel.`;
    
    return prompt;
  }
  
  async getUsageStats(): Promise<LLMUsageStats> {
    const storedStats = await this.storageRepository.get<LLMUsageStats>(STORAGE_KEYS.USAGE_STATS);
    return storedStats || this.usageStats;
  }
  
  private async generateReplyViaProxy(prompt: string, context: any): Promise<string> {
    const proxyUrl = process.env.PROXY_SERVER_URL || 'http://localhost:3000';
    console.log('[LLM] ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§LLMç”Ÿæˆé–‹å§‹');
    console.log('[LLM] ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼URL:', proxyUrl);

    const requestBody = {
      prompt: this.buildUserPrompt(prompt, context),
      context: {
        originalMessage: context?.originalMessage ? {
          from: context.originalMessage.from,
          channel: context.originalMessage.channel,
          content: context.originalMessage.content
        } : undefined,
        userPreferences: context?.userPreferences || {
          tone: 'friendly',
          language: 'ja',
          includeContext: true
        }
      }
    };

    console.log('[LLM] ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...');
    console.log('[LLM] ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ¬æ–‡:', JSON.stringify(requestBody, null, 2));
    
    const response = await fetch(`${proxyUrl}/api/llm/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody)
    });

    console.log('[LLM] ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ ãƒ¬ã‚¹ãƒãƒ³ã‚¹çŠ¶æ…‹:', response.status, response.statusText);

    if (!response.ok) {
      const errorText = await response.text();
      console.error('[LLM] ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ ã‚¨ãƒ©ãƒ¼:', response.status, errorText);
      throw new Error(`ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ ã‚¨ãƒ©ãƒ¼: ${response.status} ${errorText}`);
    }

    const data = await response.json();
    console.log('[LLM] ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ ãƒ¬ã‚¹ãƒãƒ³ã‚¹:', JSON.stringify(data, null, 2));
    
    if (!data.success || !data.reply) {
      console.error('[LLM] ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹:', data);
      throw new Error(data.error || 'ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ');
    }

    const generatedText = data.reply.trim();
    console.log('[LLM] ç”Ÿæˆã•ã‚ŒãŸè¿”ä¿¡æ¡ˆ:', generatedText);
    return generatedText;
  }

  private buildSystemPrompt(context: any): string {
    return `ã‚ãªãŸã¯æ—¥æœ¬èªã§ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã‚„å„ç¨®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¿”ä¿¡æ¡ˆã‚’ä½œæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®æŒ‡é‡ã«å¾“ã£ã¦è¿”ä¿¡æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

1. ä¸å¯§ã§è‡ªç„¶ãªæ—¥æœ¬èªã‚’ä½¿ç”¨
2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹ã‚„æ–‡è„ˆã«é©ã—ãŸè¿”ä¿¡ã‚’ç”Ÿæˆ
3. ç°¡æ½”ã§è¦ç‚¹ã‚’æŠ‘ãˆãŸå†…å®¹
4. ãƒ“ã‚¸ãƒã‚¹ã‚·ãƒ¼ãƒ³ã«é©ã—ãŸæ•¬èªã®ä½¿ç”¨
5. ç›¸æ‰‹ã¨ã®é–¢ä¿‚æ€§ã‚’è€ƒæ…®ã—ãŸé©åˆ‡ãªæ•¬èªãƒ¬ãƒ™ãƒ«

ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã®ç‰¹å¾´ï¼š
- Gmail: ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«èª¿
- Discord: å°‘ã—ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã ãŒä¸å¯§ãªå£èª¿
- LINE: è¦ªã—ã¿ã‚„ã™ãç°¡æ½”ãªè¡¨ç¾ï¼ˆçµµæ–‡å­—ä½¿ç”¨å¯ï¼‰

è¿”ä¿¡æ¡ˆã¯200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ä½œæˆã—ã¦ãã ã•ã„ã€‚`;
  }

  private buildUserPrompt(prompt: string, context: any): string {
    let userPrompt = 'ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã™ã‚‹é©åˆ‡ãªè¿”ä¿¡æ¡ˆã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n';
    
    if (context?.originalMessage) {
      const message = context.originalMessage;
      userPrompt += `**å—ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æƒ…å ±:**\n`;
      userPrompt += `é€ä¿¡è€…: ${message.from || 'ä¸æ˜'}\n`;
      userPrompt += `ãƒãƒ£ãƒ³ãƒãƒ«: ${message.channel || 'ä¸æ˜'}\n`;
      userPrompt += `å†…å®¹: ${message.content || prompt}\n\n`;
    } else {
      userPrompt += `**ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹:**\n${prompt}\n\n`;
    }
    
    userPrompt += 'é©åˆ‡ãªè¿”ä¿¡æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚';
    
    return userPrompt;
  }

  private async mockLLMGeneration(prompt: string, context: any): Promise<string> {
    // Mock implementation - simulate API call delay
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 500));
    
    // æ—¥æœ¬èªã®è¿”ä¿¡æ¡ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    const replies = [
      "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚å†…å®¹ã‚’ç¢ºèªã—ã¦ã€æ”¹ã‚ã¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚",
      "ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ã„ãŸã ã„ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã¤ã„ã¦æ¤œè¨ã•ã›ã¦ã„ãŸã ãã€å¾Œæ—¥å›ç­”ã„ãŸã—ã¾ã™ã€‚",
      "ã”é€£çµ¡ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦ã‹ã‚‰è¿”ä¿¡ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
      "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ‹è¦‹ã„ãŸã—ã¾ã—ãŸã€‚å¯¾å¿œã«ã¤ã„ã¦ãŠæ™‚é–“ã‚’ã„ãŸã ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚",
      "ãŠå¿™ã—ã„ä¸­ã”é€£çµ¡ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ç¢ºèªå¾Œã€æ”¹ã‚ã¦ãŠè¿”äº‹ã„ãŸã—ã¾ã™ã€‚",
      "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚å†…å®¹ã«ã¤ã„ã¦æ¤œè¨ã—ã¦ã€è¿‘æ—¥ä¸­ã«ãŠè¿”äº‹ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
      "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚é©åˆ‡ã«å¯¾å¿œã•ã›ã¦ã„ãŸã ãã¾ã™ã®ã§ã€å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚"
    ];
    
    // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦é©åˆ‡ãªè¿”ä¿¡ã‚’é¸æŠ
    if (context?.originalMessage) {
      const message = context.originalMessage;
      const content = message.content?.toLowerCase() || '';
      
      // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹ã«å¿œã˜ãŸè¿”ä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³
      if (content.includes('æ€¥ã') || content.includes('è‡³æ€¥') || content.includes('ç·Šæ€¥')) {
        return "ç·Šæ€¥ã®ã”é€£çµ¡ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚å„ªå…ˆçš„ã«å¯¾å¿œã•ã›ã¦ã„ãŸã ãã¾ã™ã®ã§ã€å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚";
      }
      
      if (content.includes('è³ªå•') || content.includes('æ•™ãˆã¦') || content.includes('ã©ã†')) {
        return "ã”è³ªå•ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚è©³ã—ãèª¿ã¹ã¦ãŠç­”ãˆã—ã¾ã™ã®ã§ã€ãŠæ™‚é–“ã‚’ã„ãŸã ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚";
      }
      
      if (content.includes('ã‚ã‚ŠãŒã¨ã†') || content.includes('æ„Ÿè¬')) {
        return "ã“ã¡ã‚‰ã“ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚å¼•ãç¶šãã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚";
      }
      
      if (content.includes('ä¼šè­°') || content.includes('ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°') || content.includes('æ‰“ã¡åˆã‚ã›')) {
        return "ä¼šè­°ã®ä»¶ã«ã¤ã„ã¦æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦èª¿æ•´ã„ãŸã—ã¾ã™ã€‚";
      }
      
      // ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã®è¿”ä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³
      if (message.channel === 'gmail') {
        return "ãƒ¡ãƒ¼ãƒ«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚å†…å®¹ã‚’ç¢ºèªã—ã¦ã€é©åˆ‡ã«å¯¾å¿œã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚";
      } else if (message.channel === 'discord') {
        return "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ç¢ºèªã—ã¦è¿”ä¿¡ã—ã¾ã™ã­ã€‚";
      } else if (message.channel === 'line') {
        return "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ğŸ˜Š ç¢ºèªã—ã¦ãŠè¿”äº‹ã—ã¾ã™ã­ï¼";
      }
    }
    
    return replies[Math.floor(Math.random() * replies.length)];
  }
  
  private async updateUsageStats(success: boolean, responseTime: number, tokensUsed: number): Promise<void> {
    const currentStats = await this.getUsageStats();
    
    const updatedStats: LLMUsageStats = {
      totalRequests: currentStats.totalRequests + 1,
      totalTokens: currentStats.totalTokens + tokensUsed,
      successRate: success 
        ? ((currentStats.successRate * currentStats.totalRequests) + 1) / (currentStats.totalRequests + 1)
        : (currentStats.successRate * currentStats.totalRequests) / (currentStats.totalRequests + 1),
      averageResponseTime: ((currentStats.averageResponseTime * currentStats.totalRequests) + responseTime) / (currentStats.totalRequests + 1),
      monthlyCost: currentStats.monthlyCost + (tokensUsed * 0.0001), // Rough estimation
      lastUpdated: new Date(),
    };
    
    await this.storageRepository.save(STORAGE_KEYS.USAGE_STATS, updatedStats);
    this.usageStats = updatedStats;
  }
}